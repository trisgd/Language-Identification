{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Language Identification_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJ2NpBgp_sHC",
        "outputId": "0b1bdd02-061f-4286-e39d-20ae5dd7e0ea"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnM4Qz3H_wam"
      },
      "source": [
        "#Importing the dataset\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/gdrive/My Drive/Data/sentences.csv',\n",
        "                            sep='\\t', \n",
        "                            encoding='utf8', \n",
        "                            index_col=0,\n",
        "                            names=['lang','text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn7uqXY5ABwq"
      },
      "source": [
        "filt = [True if 40<=len(s)<=500 else False for s in df['text']]\n",
        "df = df[filt]\n",
        "\n",
        "# We will train this on only 8 languages - \n",
        "lang_filter = ['deu', 'eng', 'fra', 'ita', 'por', 'spa', 'ara', 'ben']\n",
        "\n",
        "def clean_data2(data,langlist):\n",
        "  '''\n",
        "  To filter the sentences only from lang_filter\n",
        "  '''\n",
        "  data = data.loc[data['lang'].isin(langlist)]\n",
        "  return data\n",
        "\n",
        "df_new = clean_data2(df,lang_filter)\n",
        "\n",
        "#Trimming the dataset \n",
        "n = 10000\n",
        "df_red = df_new.groupby('lang').apply(lambda x: x.sample(min(n,len(x)))).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "536yHMvNAFXT",
        "outputId": "7e58bf80-946c-4eea-dff5-036075514a84"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test =  train_test_split(df_red, test_size=0.20, random_state=42)\n",
        "valid, test = train_test_split(test, test_size=0.15, random_state=30)\n",
        "\n",
        "print('Train shape: ',train.shape)\n",
        "print('Valid shape: ',valid.shape)\n",
        "print('Test shape: ',test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape:  (51940, 2)\n",
            "Valid shape:  (11038, 2)\n",
            "Test shape:  (1948, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PoAo1U0AZ6n",
        "outputId": "9d1438ef-5ea9-4c1a-a338-c8a806f14ae2"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Finding the bigrams and creating a feature set to creat the training vocabulary\n",
        "\n",
        "def bigram(text,n_feat):\n",
        "    vectorizer = CountVectorizer(analyzer='char',\n",
        "                            ngram_range=(2,2),max_features=n_feat)\n",
        "    \n",
        "    X = vectorizer.fit_transform(text)\n",
        "    feature_names = vectorizer.get_feature_names()    \n",
        "    return feature_names\n",
        "\n",
        "features = {}\n",
        "features_set = set()\n",
        "\n",
        "for l in lang_filter:\n",
        "    corpus = train[train.lang==l]['text']\n",
        "    bigrams = bigram(corpus,250)\n",
        "    features[l] = bigrams\n",
        "    features_set.update(bigrams)\n",
        "\n",
        "# Vocab created to feed into the Count Vectorizer for training data\n",
        "vocab = dict()\n",
        "for i,feat in enumerate(features_set):\n",
        "    vocab[feat]=i\n",
        "print('Vocab')\n",
        "print(vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab\n",
            "{'রব': 0, 'y.': 1, 'w ': 2, 'i,': 3, 'c ': 4, 'إل': 5, 'ed': 6, 'ট ': 7, 'েষ': 8, 'ঙ্': 9, 'hu': 10, ' ছ': 11, 'un': 12, ' r': 13, 'بة': 14, 'رو': 15, 'يو': 16, 'ul': 17, 'لي': 18, 'ej': 19, 's.': 20, 'هي': 21, 'wu': 22, 'كث': 23, 'عي': 24, 'ré': 25, 'oy': 26, '، ': 27, 'া।': 28, ' à': 29, 'ন?': 30, 'مت': 31, 'ُ ': 32, ' m': 33, 'h,': 34, 'فر': 35, 'لو': 36, 'লো': 37, 'ei': 38, 'له': 39, 'ér': 40, 'চা': 41, 'ow': 42, 'oa': 43, 'কি': 44, 'z ': 45, 'sc': 46, 'োম': 47, 'াট': 48, 'াঁ': 49, 'rt': 50, 'ا.': 51, 'كي': 52, 'খা': 53, 'ণ ': 54, 'াছ': 55, ' ك': 56, ' u': 57, 'wh': 58, \"'s\": 59, 'nh': 60, 'عم': 61, 'ছা': 62, 'rò': 63, 'ال': 64, 'تر': 65, 'ন ': 66, 'ep': 67, 'hö': 68, 'ls': 69, ' ন': 70, 'fa': 71, 'wo': 72, 'nt': 73, 'نّ': 74, 'de': 75, 'tà': 76, 'س ': 77, 'মা': 78, 'eb': 79, '\"i': 80, 'tu': 81, 'll': 82, '্ষ': 83, 'am': 84, 'كر': 85, 'ju': 86, ' ঘ': 87, 'má': 88, 'tw': 89, 'াব': 90, 'টে': 91, 'rg': 92, 'ية': 93, ' প': 94, 'نه': 95, 'اف': 96, ' ش': 97, 'ء ': 98, 'ت ': 99, 'e?': 100, 'ko': 101, 'rà': 102, 'be': 103, 'ن ': 104, 'ব ': 105, 'ف ': 106, 'حق': 107, 'ন্': 108, 'rd': 109, 'اد': 110, 'il': 111, 'bb': 112, 'pr': 113, ' গ': 114, 'eu': 115, 'ن.': 116, 'রত': 117, 'ান': 118, 'قت': 119, 'h ': 120, ' f': 121, 'tz': 122, 'mã': 123, 'ti': 124, 'nç': 125, 'pi': 126, 'om': 127, 'تي': 128, 'ta': 129, 'ct': 130, 'ib': 131, 'o?': 132, 'ما': 133, 'at': 134, 'اً': 135, 'èr': 136, 'বা': 137, ' ভ': 138, 'সম': 139, 'হা': 140, 'bt': 141, 'uv': 142, 'খন': 143, 'েখ': 144, 's,': 145, 'رب': 146, 'ba': 147, 'éc': 148, 'ni': 149, 'n?': 150, 'ছি': 151, 'িৎ': 152, 'se': 153, 'iu': 154, 'd.': 155, 'ih': 156, 'لل': 157, ' ত': 158, 'ik': 159, 'sn': 160, 'az': 161, 'ch': 162, 'ان': 163, 'يل': 164, ' শ': 165, 'হ ': 166, 'n,': 167, 'أو': 168, 'و ': 169, 'ty': 170, 'بر': 171, 'جم': 172, 'rk': 173, 'নে': 174, 'ef': 175, ' y': 176, 'টু': 177, 'লত': 178, 'pu': 179, 'bo': 180, 'rè': 181, 'ند': 182, 'oh': 183, 'اك': 184, 'بو': 185, 'لش': 186, 'ha': 187, 'gs': 188, 'rü': 189, 'জ ': 190, 'ني': 191, 'া ': 192, 'gu': 193, 'نا': 194, 'th': 195, 'ät': 196, 'ছে': 197, 'zu': 198, 'uy': 199, 'ি ': 200, 'صا': 201, 'e,': 202, 'rz': 203, 'جد': 204, ' z': 205, 'ا ': 206, 'কা': 207, 'hl': 208, 'fr': 209, 'ুম': 210, 'is': 211, 'াথ': 212, 'ো।': 213, ' h': 214, 'وا': 215, 'gl': 216, 'েম': 217, \"'e\": 218, ' c': 219, ' ü': 220, 'يد': 221, 'يت': 222, 'dn': 223, 'أم': 224, 'াল': 225, 'ع ': 226, 'عن': 227, 'ر ': 228, 'ge': 229, ' غ': 230, 'üc': 231, 'ès': 232, 'أخ': 233, 'بل': 234, ' ر': 235, 'ée': 236, 'ue': 237, 'রহ': 238, 'o,': 239, 'of': 240, 's-': 241, '? ': 242, 'لغ': 243, 'y ': 244, 'رة': 245, 'hr': 246, 'ডা': 247, 'i ': 248, 'সে': 249, ', ': 250, 'ু ': 251, ' ج': 252, 'إن': 253, 'গ্': 254, 'ضل': 255, 'িট': 256, 'مل': 257, 'nz': 258, 'من': 259, 'as': 260, 'i.': 261, 'بع': 262, 'বল': 263, 'n ': 264, 'যা': 265, ' p': 266, 'my': 267, 'أك': 268, 'ির': 269, 'াত': 270, 'po': 271, 'eg': 272, 'ex': 273, ' أ': 274, 'rb': 275, 'cu': 276, 'eh': 277, 'ee': 278, 'تق': 279, ' স': 280, 'মধ': 281, ' ل': 282, 'ma': 283, 'ué': 284, 'sã': 285, 'av': 286, 'وي': 287, 'g ': 288, 'kö': 289, 'ht': 290, 'm ': 291, 'io': 292, 'ka': 293, 'od': 294, 'ó ': 295, 'اع': 296, 'ês': 297, 'ে?': 298, 'vi': 299, 'än': 300, 'rs': 301, '-l': 302, 'iè': 303, 'أر': 304, 'ud': 305, 'به': 306, \"l'\": 307, 'ft': 308, '় ': 309, 'ey': 310, 'ça': 311, 'di': 312, 'ét': 313, ' ধ': 314, 'াও': 315, 'za': 316, 'im': 317, 'ই ': 318, 'াম': 319, 'লে': 320, ' ম': 321, 'ময': 322, 'o.': 323, 'েউ': 324, ' অ': 325, 'খু': 326, 'عة': 327, ' খ': 328, 'دة': 329, 'ua': 330, ' t': 331, 'স ': 332, 'র ': 333, 'ড়': 334, ' দ': 335, 'nd': 336, 'ذي': 337, 'নু': 338, 'és': 339, 'sa': 340, 'nk': 341, 'ós': 342, 'اب': 343, 'خر': 344, 'দি': 345, \"u'\": 346, 'মে': 347, 'zt': 348, 'অপ': 349, 'gn': 350, 'িজ': 351, 'ه ': 352, 'ুর': 353, 'ات': 354, 'نس': 355, 'সঙ': 356, 'ম।': 357, 'me': 358, 'দা': 359, 'si': 360, 'يب': 361, 'so': 362, 'حت': 363, ' \"': 364, 'অন': 365, 'يه': 366, 'مس': 367, ' i': 368, 'ষা': 369, 'ুন': 370, 'op': 371, 'لق': 372, 'ب ': 373, 'ঞা': 374, '়ি': 375, 'oo': 376, 'tã': 377, 'ro': 378, '। ': 379, 't,': 380, 'تم': 381, 'hw': 382, 't-': 383, 'ot': 384, 'ওয': 385, 'oe': 386, 'تح': 387, 'ছু': 388, 'ço': 389, ' s': 390, 'ম ': 391, 'যে': 392, 'nn': 393, 'ea': 394, 'ny': 395, 'প্': 396, 'te': 397, ' ا': 398, 'ap': 399, 'äh': 400, 'ত ': 401, 'go': 402, 'lh': 403, 'لد': 404, 'مة': 405, 'ি।': 406, 'أي': 407, 'id': 408, 'ست': 409, 'ً ': 410, ' ب': 411, 'শি': 412, 'ল ': 413, 'aj': 414, 'চ্': 415, 'ুব': 416, 'to': 417, 'ce': 418, 'ze': 419, 'থে': 420, 'l ': 421, 'أن': 422, 'ff': 423, 'iz': 424, ' ق': 425, 'دا': 426, 'ى ': 427, 'طي': 428, 'ya': 429, 'مر': 430, 'يك': 431, 'fü': 432, 'সি': 433, 'িল': 434, 'rí': 435, 'ين': 436, 'gr': 437, 'াক': 438, 'ia': 439, 'سي': 440, 'َ ': 441, 'co': 442, 'e.': 443, ' জ': 444, 'لس': 445, 'nf': 446, ' ক': 447, 'pl': 448, 'ev': 449, 'or': 450, 'él': 451, 'عد': 452, 'শ্': 453, '্ব': 454, 'ne': 455, 'দে': 456, 'ov': 457, 'িয': 458, ' k': 459, 'ci': 460, 'ob': 461, 'rc': 462, '.\"': 463, ' a': 464, 'عت': 465, 'دو': 466, 'مع': 467, 'ki': 468, 'u ': 469, 'মর': 470, ' o': 471, 'د ': 472, 'قي': 473, 'ou': 474, 'াদ': 475, 'েল': 476, 'al': 477, 'vr': 478, 'আর': 479, 'تك': 480, 'án': 481, 'ép': 482, 'tt': 483, 'ok': 484, 'কথ': 485, 'ح ': 486, 'কট': 487, 'à ': 488, 'কর': 489, 'ke': 490, 'ون': 491, 'ck': 492, 'çã': 493, 'lc': 494, ' উ': 495, 'স্': 496, 'র্': 497, ' ?': 498, 'يّ': 499, 'সা': 500, 'añ': 501, 'st': 502, 'وق': 503, 'rl': 504, 'بي': 505, ' b': 506, 'ás': 507, 'ود': 508, 'িশ': 509, 'كن': 510, ' এ': 511, 'x ': 512, 'ri': 513, 'িক': 514, 'if': 515, 'اض': 516, 'mo': 517, 'রক': 518, 'ো ': 519, 'ür': 520, 'টম': 521, '়ে': 522, 'jo': 523, 'পে': 524, 'েত': 525, 'aq': 526, 'য়': 527, 'েব': 528, 'েক': 529, 'েট': 530, 'kl': 531, 'an': 532, ' d': 533, \"'u\": 534, 'ে ': 535, 'زي': 536, 'ام': 537, 'ة ': 538, 'قو': 539, 'يق': 540, 'hn': 541, 'rf': 542, 'ui': 543, 'lé': 544, 'es': 545, 'k ': 546, 'cc': 547, 'nã': 548, 'ew': 549, ' م': 550, 'êt': 551, 'p ': 552, 'ত্': 553, 'নো': 554, 'up': 555, 'اي': 556, 'gg': 557, 'در': 558, 'a,': 559, 'hé': 560, 'mé': 561, 'كل': 562, ' ن': 563, 'গে': 564, 'ld': 565, 'ভা': 566, 'هذ': 567, 'zz': 568, 'ol': 569, 'o ': 570, 'lm': 571, 'سب': 572, 'كا': 573, 'üb': 574, 'cr': 575, 'لّ': 576, 'েন': 577, 'পন': 578, '্ট': 579, 'ys': 580, 'hi': 581, ' w': 582, 'us': 583, \"'a\": 584, 'ò ': 585, ' خ': 586, 'حا': 587, 'gt': 588, 'هو': 589, 'লি': 590, 'ss': 591, 'ak': 592, '্ধ': 593, 'রা': 594, 'dr': 595, 'tá': 596, 'িছ': 597, 'we': 598, 'سا': 599, 'ّا': 600, 'hä': 601, 'né': 602, 'لإ': 603, ' থ': 604, 'fi': 605, 'াস': 606, 'kt': 607, 'ير': 608, 'يا': 609, '. ': 610, 'ts': 611, 'bl': 612, 'ry': 613, 'اح': 614, 'لا': 615, 'bu': 616, 'sm': 617, ' ع': 618, 'i?': 619, 'لم': 620, 'ön': 621, 'পর': 622, 'iù': 623, 'চি': 624, 'ir': 625, '্ক': 626, 'af': 627, 'dí': 628, 'ائ': 629, 'رج': 630, ' ব': 631, 'em': 632, 'কে': 633, 'ab': 634, ' n': 635, 'rá': 636, 'اس': 637, 'জি': 638, 'টি': 639, 'ته': 640, 'রি': 641, 'عا': 642, 'نت': 643, ' ফ': 644, 'حي': 645, 'জ্': 646, 'do': 647, 'ño': 648, 'কো': 649, '্ত': 650, 'su': 651, 'فا': 652, 'ij': 653, 'عر': 654, 'ন,': 655, 'াড': 656, 'no': 657, 'বি': 658, 'da': 659, 'et': 660, 'mb': 661, 'ذا': 662, 'لة': 663, ' র': 664, ' ল': 665, 'ার': 666, 'í ': 667, 'lg': 668, 'è ': 669, 'شر': 670, 'ধ্': 671, 'er': 672, ' l': 673, 'يس': 674, 'ió': 675, 'لك': 676, 'ফর': 677, 'ub': 678, 'هم': 679, 'নত': 680, 'ez': 681, 'م ': 682, 'رف': 683, 'ور': 684, 'ো,': 685, 'le': 686, 'ng': 687, 'na': 688, 'ug': 689, 'তে': 690, 'je': 691, 'd ': 692, 'gh': 693, 'েই': 694, 'ru': 695, 'كو': 696, 'غي': 697, 'oi': 698, 'ut': 699, 'لن': 700, 'ক ': 701, 'لأ': 702, 'মন': 703, 'াপ': 704, 'iq': 705, 'রো': 706, 'مك': 707, 'খে': 708, 'en': 709, \"c'\": 710, 'ía': 711, ' ت': 712, 'এক': 713, ' v': 714, 'ù ': 715, ' q': 716, 'ق ': 717, 'هن': 718, 'n.': 719, 'لع': 720, 'sh': 721, 'ra': 722, 'ক্': 723, 'ns': 724, 'ম্': 725, 'ষ্': 726, 't ': 727, ' e': 728, 'ه.': 729, 'تو': 730, 'au': 731, 'lt': 732, \"'o\": 733, 'á ': 734, 'ie': 735, 'উ ': 736, 'br': 737, 'قد': 738, 'ay': 739, 'oc': 740, \"t'\": 741, \"s'\": 742, 'rr': 743, 'ga': 744, 'مو': 745, ' ف': 746, 'হয': 747, 'sé': 748, 'a?': 749, 'না': 750, 'ar': 751, 'uc': 752, '্য': 753, 'لذ': 754, 'اء': 755, 'a.': 756, 'রে': 757, 'ec': 758, 's ': 759, 'dé': 760, 'তো': 761, 'ار': 762, 'ao': 763, 'পা': 764, 'قة': 765, 'du': 766, 'আপ': 767, 'el': 768, 'kn': 769, 'ah': 770, 'تل': 771, 'ّة': 772, 'cl': 773, 'জন': 774, 'বে': 775, 'gi': 776, 'ة.': 777, 'جا': 778, 'নি': 779, 'a ': 780, 'شي': 781, 'é ': 782, 'এই': 783, '্ঞ': 784, 'لت': 785, 'wa': 786, 'دي': 787, ' g': 788, 'ذه': 789, 'مي': 790, 'wi': 791, 'ই।': 792, 'f ': 793, ' إ': 794, 'ل ': 795, 'vo': 796, 'aç': 797, 'েছ': 798, 'তা': 799, 'ly': 800, 'he': 801, 'on': 802, 'la': 803, 'rm': 804, 'ye': 805, 'تع': 806, 'va': 807, 're': 808, 'ár': 809, 'ón': 810, ' চ': 811, ' ট': 812, 'mp': 813, 'ip': 814, 'াই': 815, 'يع': 816, 'li': 817, \"'i\": 818, ' ح': 819, 'ও ': 820, ' ه': 821, 'থা': 822, 'pp': 823, 'os': 824, ' ড': 825, 'শে': 826, \"j'\": 827, ' j': 828, 'نو': 829, 'বো': 830, 'ها': 831, \"n'\": 832, \"e'\": 833, 'lo': 834, 'mi': 835, 'মি': 836, ' আ': 837, 'eç': 838, 'e ': 839, 'مد': 840, 'এখ': 841, \"i'\": 842, 'ße': 843, 'uo': 844, 'টা': 845, 'ão': 846, 'fe': 847, ' س': 848, 'أس': 849, 'কল': 850, 'mu': 851, 'ig': 852, ' হ': 853, 'ca': 854, ' و': 855, 'اج': 856, 'ك ': 857, ' য': 858, 'েয': 859, 'og': 860, 'بد': 861, 'ic': 862, 'اق': 863, 'لف': 864, 'এট': 865, 'ém': 866, 've': 867, 'ّ ': 868, ' د': 869, 'اه': 870, 'يم': 871, 'ের': 872, 'ur': 873, 't.': 874, 'r ': 875, 'সব': 876, 'চে': 877, '়া': 878, 'üh': 879, 'িন': 880, 'ي ': 881, \"d'\": 882, '্র': 883, 'ho': 884, 'ix': 885, 'عل': 886, 'r,': 887, \"'t\": 888, 'سل': 889, 'لى': 890, 'bi': 891, 'لج': 892, 'াজ': 893, 'ux': 894, 'ায': 895, 'by': 896, 'l.': 897, 'ন।': 898, 'তু': 899, 'uf': 900, 'lu': 901, 'را': 902, 'لر': 903, \"'é\": 904, 'um': 905, 'it': 906, 'nu': 907, 'fu': 908, 'আম': 909, 'ag': 910, 'قا': 911, 'eo': 912, 'ري': 913, 'ুগ': 914, 'هل': 915, 'حد': 916, 'cê': 917, 'لح': 918, 'rn': 919, 'yo': 920, 'ad': 921, 'ac': 922, 'fo': 923, 'ja': 924, 'té': 925, 'ض ': 926, 'ai': 927, 'ول': 928, 'রছ': 929, 'লা': 930, 'تا': 931, 'বর': 932, 'r.': 933, 'sp': 934, 'g.': 935, 'tr': 936, 'ব্': 937, ' ও': 938, 'm.': 939, 'لب': 940, 'نف': 941, 'hm': 942, 'ña': 943, 'উচ': 944, 'nc': 945, 'pe': 946, 'ds': 947, 'في': 948, 'জা': 949, 'وم': 950, 'তি': 951, 'pa': 952, 'ي.': 953, 'য ': 954, 'تن': 955, 'qu': 956, ' ي': 957, ' é': 958, 'োন': 959, '্গ': 960, 'in': 961, 'iv': 962, 'با': 963, 'دم': 964, 'ek': 965, 'zi': 966, ' ص': 967, 'হব': 968, 'িত': 969, ' è': 970, 'ê ': 971, 'mm': 972, 'ে।': 973}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecrzmWJ3Ax1U"
      },
      "source": [
        "vectorizer = CountVectorizer(analyzer='char',\n",
        "                             ngram_range=(2, 2),\n",
        "                            vocabulary=vocab)\n",
        "\n",
        "# Creating the train feature matrix \n",
        "data_x = train['text']   \n",
        "X = vectorizer.fit_transform(data_x)\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "\n",
        "train_features = pd.DataFrame(data=X.toarray(),columns=feature_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "Y3I0xTW9A_W1",
        "outputId": "136ba766-376e-4fba-f6ae-a56240be6fab"
      },
      "source": [
        "print(train_features)\n",
        "'''\n",
        "The columns in train_feat tells you about list of bigrams and the rows represent the sentences \n",
        "The zeros and ones indicate the presence or absence of the particular bigram in the sentence\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       রব  y.  w   i,  c   إل  ed  ট   েষ  ...  ek  zi   ص  হব  িত   è  ê   mm  ে।\n",
            "0       0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0\n",
            "1       0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0\n",
            "2       0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0\n",
            "3       0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   1   0\n",
            "4       0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0\n",
            "...    ..  ..  ..  ..  ..  ..  ..  ..  ..  ...  ..  ..  ..  ..  ..  ..  ..  ..  ..\n",
            "51935   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0\n",
            "51936   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0\n",
            "51937   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0\n",
            "51938   0   0   0   0   0   0   1   0   0  ...   0   0   0   0   0   0   0   0   0\n",
            "51939   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0\n",
            "\n",
            "[51940 rows x 974 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nThe columns in train_feat tells you about list of bigrams and the rows represent the sentences \\nThe zeros and ones indicate the presence or absence of the particular bigram in the sentence\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKmY0nLdBChI"
      },
      "source": [
        "def normalized(train_features, feat):\n",
        "  tr_min = train_features.min()\n",
        "  tr_max = train_features.max()\n",
        "  feat = (feat - tr_min)/(tr_max-tr_min)\n",
        "  return feat\n",
        "\n",
        "train_features_norm = normalized(train_features,train_features)\n",
        "\n",
        "#Add target variable \n",
        "train_features_norm['lang_target'] = list(train['lang'])\n",
        "\n",
        "# Data Processing for the test and validation set\n",
        "\n",
        "valid_text = valid['text']   \n",
        "X = vectorizer.fit_transform(valid_text)\n",
        "\n",
        "valid_features = pd.DataFrame(data=X.toarray(),columns=feature_names)\n",
        "valid_features = normalized(train_features,valid_features)\n",
        "valid_features['lang_target'] = list(valid['lang'])\n",
        "\n",
        "\n",
        "test_text = test['text']   \n",
        "X = vectorizer.fit_transform(test_text)\n",
        "\n",
        "test_features = pd.DataFrame(data=X.toarray(),columns=feature_names)\n",
        "test_features = normalized(train_features, test_features)\n",
        "test_features['lang_target'] = list(test['lang'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40XwliWpBUWP",
        "outputId": "1d3c3d11-e43d-4d10-dd10-6a19669922a3"
      },
      "source": [
        "print(test_features.shape)\n",
        "print(valid_features.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1948, 975)\n",
            "(11038, 975)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxikeyShBYNc",
        "outputId": "61c3e7d1-4b64-467f-b1b0-623a70501697"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import np_utils\n",
        "\n",
        "lang_filter = ['deu', 'eng', 'fra', 'ita', 'por', 'spa', 'ara', 'ben']\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(lang_filter)\n",
        "\n",
        "X_train = train_features_norm.drop('lang_target',axis = 1)\n",
        "Y_train = np_utils.to_categorical(encoder.transform(train_features_norm['lang_target']))\n",
        "\n",
        "X_val = valid_features.drop('lang_target',axis = 1)\n",
        "Y_val = np_utils.to_categorical(encoder.transform(valid_features['lang_target']))\n",
        "\n",
        "X_test = test_features.drop('lang_target',axis = 1)\n",
        "Y_test = test_features['lang_target']\n",
        "Y_test = np_utils.to_categorical(encoder.transform(test_features['lang_target']))\n",
        "\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_val.shape,Y_val.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(51940, 974) (51940, 8)\n",
            "(11038, 974) (11038, 8)\n",
            "(1948, 974) (1948, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xi_g5Ra2CG7n",
        "outputId": "ff7433a8-5a3b-43a1-e152-c06c2a4b8e08"
      },
      "source": [
        "import numpy as np\n",
        "X_train = np.expand_dims(X_train, axis=1)\n",
        "X_val = np.expand_dims(X_val,axis=1)\n",
        "X_test = np.expand_dims(X_test, axis=1)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(51940, 1, 974)\n",
            "(11038, 1, 974)\n",
            "(1948, 1, 974)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42vKJcjJCYA5"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
        "import keras\n",
        "\n",
        "def evaluate_model(trainX, trainy, testX, testy):\n",
        "  verbose, epochs, batch_size = 1, 10, 32\n",
        "  n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
        "  model = Sequential()\n",
        "  model.add(Conv1D(filters=64, kernel_size=1, activation='relu', input_shape=(n_timesteps,n_features)))\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(Conv1D(filters=64, kernel_size=1, activation='relu'))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(100, activation='relu'))\n",
        "  model.add(Dense(n_outputs, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, validation_data=(X_val, Y_val), verbose=verbose)\n",
        "  _, accuracy = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=0)\n",
        "  return accuracy\n",
        "\n",
        "def summarize_results(scores):\n",
        "\tprint(scores)\n",
        " \n",
        "\n",
        "def run_experiment(repeats,trainX,trainy,testX,testy):\n",
        "\tscores = list()\n",
        "\tfor r in range(repeats):\n",
        "\t\tscore = evaluate_model(trainX, trainy, testX, testy)\n",
        "\t\tscore = score * 100.0\n",
        "\t\tprint('>#%d: %.3f' % (r+1, score))\n",
        "\t\tscores.append(score)\n",
        "\tsummarize_results(scores)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1fuIm14CurN",
        "outputId": "44dcf558-61d6-45f2-ed77-0ece7f2c5ac2"
      },
      "source": [
        "run_experiment(10,X_train,Y_train,X_test,Y_test)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.3897 - accuracy: 0.8707 - val_loss: 0.0283 - val_accuracy: 0.9908\n",
            "Epoch 2/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0221 - accuracy: 0.9931 - val_loss: 0.0189 - val_accuracy: 0.9943\n",
            "Epoch 3/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0152 - accuracy: 0.9944 - val_loss: 0.0197 - val_accuracy: 0.9932\n",
            "Epoch 4/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0110 - accuracy: 0.9961 - val_loss: 0.0187 - val_accuracy: 0.9951\n",
            "Epoch 5/10\n",
            "1624/1624 [==============================] - 4s 3ms/step - loss: 0.0094 - accuracy: 0.9966 - val_loss: 0.0244 - val_accuracy: 0.9938\n",
            "Epoch 6/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0076 - accuracy: 0.9976 - val_loss: 0.0245 - val_accuracy: 0.9935\n",
            "Epoch 7/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0068 - accuracy: 0.9979 - val_loss: 0.0239 - val_accuracy: 0.9940\n",
            "Epoch 8/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0055 - accuracy: 0.9981 - val_loss: 0.0284 - val_accuracy: 0.9937\n",
            "Epoch 9/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0052 - accuracy: 0.9980 - val_loss: 0.0272 - val_accuracy: 0.9940\n",
            "Epoch 10/10\n",
            "1624/1624 [==============================] - 4s 3ms/step - loss: 0.0026 - accuracy: 0.9991 - val_loss: 0.0250 - val_accuracy: 0.9949\n",
            ">#1: 99.333\n",
            "Epoch 1/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.3946 - accuracy: 0.8826 - val_loss: 0.0234 - val_accuracy: 0.9928\n",
            "Epoch 2/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0218 - accuracy: 0.9928 - val_loss: 0.0197 - val_accuracy: 0.9934\n",
            "Epoch 3/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0155 - accuracy: 0.9950 - val_loss: 0.0188 - val_accuracy: 0.9938\n",
            "Epoch 4/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0108 - accuracy: 0.9964 - val_loss: 0.0190 - val_accuracy: 0.9943\n",
            "Epoch 5/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0087 - accuracy: 0.9971 - val_loss: 0.0253 - val_accuracy: 0.9925\n",
            "Epoch 6/10\n",
            "1624/1624 [==============================] - 4s 3ms/step - loss: 0.0078 - accuracy: 0.9971 - val_loss: 0.0254 - val_accuracy: 0.9935\n",
            "Epoch 7/10\n",
            "1624/1624 [==============================] - 4s 3ms/step - loss: 0.0068 - accuracy: 0.9977 - val_loss: 0.0286 - val_accuracy: 0.9924\n",
            "Epoch 8/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0047 - accuracy: 0.9982 - val_loss: 0.0261 - val_accuracy: 0.9933\n",
            "Epoch 9/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0046 - accuracy: 0.9986 - val_loss: 0.0304 - val_accuracy: 0.9922\n",
            "Epoch 10/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0028 - accuracy: 0.9991 - val_loss: 0.0364 - val_accuracy: 0.9917\n",
            ">#2: 99.487\n",
            "Epoch 1/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.3923 - accuracy: 0.8795 - val_loss: 0.0216 - val_accuracy: 0.9933\n",
            "Epoch 2/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0209 - accuracy: 0.9932 - val_loss: 0.0198 - val_accuracy: 0.9940\n",
            "Epoch 3/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0149 - accuracy: 0.9955 - val_loss: 0.0193 - val_accuracy: 0.9942\n",
            "Epoch 4/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0106 - accuracy: 0.9967 - val_loss: 0.0200 - val_accuracy: 0.9934\n",
            "Epoch 5/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0094 - accuracy: 0.9969 - val_loss: 0.0218 - val_accuracy: 0.9932\n",
            "Epoch 6/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0068 - accuracy: 0.9977 - val_loss: 0.0223 - val_accuracy: 0.9938\n",
            "Epoch 7/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0057 - accuracy: 0.9976 - val_loss: 0.0260 - val_accuracy: 0.9938\n",
            "Epoch 8/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0036 - accuracy: 0.9989 - val_loss: 0.0256 - val_accuracy: 0.9932\n",
            "Epoch 9/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0036 - accuracy: 0.9987 - val_loss: 0.0340 - val_accuracy: 0.9926\n",
            "Epoch 10/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0028 - accuracy: 0.9991 - val_loss: 0.0292 - val_accuracy: 0.9934\n",
            ">#3: 99.435\n",
            "Epoch 1/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.3931 - accuracy: 0.8772 - val_loss: 0.0222 - val_accuracy: 0.9930\n",
            "Epoch 2/10\n",
            "1624/1624 [==============================] - 4s 3ms/step - loss: 0.0198 - accuracy: 0.9938 - val_loss: 0.0205 - val_accuracy: 0.9941\n",
            "Epoch 3/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0153 - accuracy: 0.9946 - val_loss: 0.0285 - val_accuracy: 0.9921\n",
            "Epoch 4/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0115 - accuracy: 0.9957 - val_loss: 0.0257 - val_accuracy: 0.9923\n",
            "Epoch 5/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0081 - accuracy: 0.9974 - val_loss: 0.0241 - val_accuracy: 0.9928\n",
            "Epoch 6/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0070 - accuracy: 0.9977 - val_loss: 0.0335 - val_accuracy: 0.9912\n",
            "Epoch 7/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0064 - accuracy: 0.9979 - val_loss: 0.0298 - val_accuracy: 0.9925\n",
            "Epoch 8/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0038 - accuracy: 0.9988 - val_loss: 0.0313 - val_accuracy: 0.9941\n",
            "Epoch 9/10\n",
            "1624/1624 [==============================] - 6s 4ms/step - loss: 0.0049 - accuracy: 0.9983 - val_loss: 0.0433 - val_accuracy: 0.9910\n",
            "Epoch 10/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0039 - accuracy: 0.9988 - val_loss: 0.0327 - val_accuracy: 0.9928\n",
            ">#4: 99.538\n",
            "Epoch 1/10\n",
            "1624/1624 [==============================] - 6s 3ms/step - loss: 0.3852 - accuracy: 0.8863 - val_loss: 0.0290 - val_accuracy: 0.9914\n",
            "Epoch 2/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0214 - accuracy: 0.9930 - val_loss: 0.0297 - val_accuracy: 0.9900\n",
            "Epoch 3/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0145 - accuracy: 0.9946 - val_loss: 0.0268 - val_accuracy: 0.9921\n",
            "Epoch 4/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0124 - accuracy: 0.9954 - val_loss: 0.0260 - val_accuracy: 0.9918\n",
            "Epoch 5/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0084 - accuracy: 0.9970 - val_loss: 0.0269 - val_accuracy: 0.9925\n",
            "Epoch 6/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0061 - accuracy: 0.9978 - val_loss: 0.0295 - val_accuracy: 0.9925\n",
            "Epoch 7/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0047 - accuracy: 0.9982 - val_loss: 0.0301 - val_accuracy: 0.9930\n",
            "Epoch 8/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0051 - accuracy: 0.9984 - val_loss: 0.0281 - val_accuracy: 0.9931\n",
            "Epoch 9/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.9988 - val_loss: 0.0305 - val_accuracy: 0.9930\n",
            "Epoch 10/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0030 - accuracy: 0.9991 - val_loss: 0.0335 - val_accuracy: 0.9919\n",
            ">#5: 99.333\n",
            "Epoch 1/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.3890 - accuracy: 0.8758 - val_loss: 0.0238 - val_accuracy: 0.9927\n",
            "Epoch 2/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0197 - accuracy: 0.9935 - val_loss: 0.0232 - val_accuracy: 0.9935\n",
            "Epoch 3/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0162 - accuracy: 0.9944 - val_loss: 0.0284 - val_accuracy: 0.9919\n",
            "Epoch 4/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0115 - accuracy: 0.9961 - val_loss: 0.0259 - val_accuracy: 0.9931\n",
            "Epoch 5/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0095 - accuracy: 0.9965 - val_loss: 0.0289 - val_accuracy: 0.9920\n",
            "Epoch 6/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0075 - accuracy: 0.9974 - val_loss: 0.0260 - val_accuracy: 0.9928\n",
            "Epoch 7/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0048 - accuracy: 0.9982 - val_loss: 0.0302 - val_accuracy: 0.9937\n",
            "Epoch 8/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0047 - accuracy: 0.9983 - val_loss: 0.0278 - val_accuracy: 0.9927\n",
            "Epoch 9/10\n",
            "1624/1624 [==============================] - 4s 3ms/step - loss: 0.0059 - accuracy: 0.9979 - val_loss: 0.0316 - val_accuracy: 0.9931\n",
            "Epoch 10/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0035 - accuracy: 0.9988 - val_loss: 0.0303 - val_accuracy: 0.9937\n",
            ">#6: 99.384\n",
            "Epoch 1/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.3837 - accuracy: 0.8931 - val_loss: 0.0220 - val_accuracy: 0.9930\n",
            "Epoch 2/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0229 - accuracy: 0.9925 - val_loss: 0.0204 - val_accuracy: 0.9932\n",
            "Epoch 3/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0158 - accuracy: 0.9951 - val_loss: 0.0243 - val_accuracy: 0.9922\n",
            "Epoch 4/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0128 - accuracy: 0.9953 - val_loss: 0.0216 - val_accuracy: 0.9940\n",
            "Epoch 5/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0086 - accuracy: 0.9969 - val_loss: 0.0245 - val_accuracy: 0.9935\n",
            "Epoch 6/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0075 - accuracy: 0.9972 - val_loss: 0.0467 - val_accuracy: 0.9876\n",
            "Epoch 7/10\n",
            "1624/1624 [==============================] - 7s 4ms/step - loss: 0.0074 - accuracy: 0.9972 - val_loss: 0.0253 - val_accuracy: 0.9928\n",
            "Epoch 8/10\n",
            "1624/1624 [==============================] - 6s 4ms/step - loss: 0.0036 - accuracy: 0.9987 - val_loss: 0.0300 - val_accuracy: 0.9932\n",
            "Epoch 9/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0031 - accuracy: 0.9989 - val_loss: 0.0324 - val_accuracy: 0.9928\n",
            "Epoch 10/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0037 - accuracy: 0.9989 - val_loss: 0.0391 - val_accuracy: 0.9915\n",
            ">#7: 99.384\n",
            "Epoch 1/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.3904 - accuracy: 0.8624 - val_loss: 0.0230 - val_accuracy: 0.9931\n",
            "Epoch 2/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0217 - accuracy: 0.9926 - val_loss: 0.0233 - val_accuracy: 0.9924\n",
            "Epoch 3/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0160 - accuracy: 0.9949 - val_loss: 0.0242 - val_accuracy: 0.9920\n",
            "Epoch 4/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0136 - accuracy: 0.9954 - val_loss: 0.0243 - val_accuracy: 0.9926\n",
            "Epoch 5/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0083 - accuracy: 0.9970 - val_loss: 0.0206 - val_accuracy: 0.9930\n",
            "Epoch 6/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0065 - accuracy: 0.9977 - val_loss: 0.0237 - val_accuracy: 0.9935\n",
            "Epoch 7/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0060 - accuracy: 0.9979 - val_loss: 0.0292 - val_accuracy: 0.9932\n",
            "Epoch 8/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0044 - accuracy: 0.9984 - val_loss: 0.0271 - val_accuracy: 0.9937\n",
            "Epoch 9/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0042 - accuracy: 0.9985 - val_loss: 0.0328 - val_accuracy: 0.9931\n",
            "Epoch 10/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.9989 - val_loss: 0.0270 - val_accuracy: 0.9941\n",
            ">#8: 99.589\n",
            "Epoch 1/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.3822 - accuracy: 0.8883 - val_loss: 0.0221 - val_accuracy: 0.9918\n",
            "Epoch 2/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0223 - accuracy: 0.9930 - val_loss: 0.0241 - val_accuracy: 0.9929\n",
            "Epoch 3/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0145 - accuracy: 0.9952 - val_loss: 0.0187 - val_accuracy: 0.9938\n",
            "Epoch 4/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0111 - accuracy: 0.9958 - val_loss: 0.0210 - val_accuracy: 0.9938\n",
            "Epoch 5/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0085 - accuracy: 0.9976 - val_loss: 0.0245 - val_accuracy: 0.9919\n",
            "Epoch 6/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0057 - accuracy: 0.9980 - val_loss: 0.0228 - val_accuracy: 0.9936\n",
            "Epoch 7/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0050 - accuracy: 0.9983 - val_loss: 0.0227 - val_accuracy: 0.9932\n",
            "Epoch 8/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0042 - accuracy: 0.9986 - val_loss: 0.0236 - val_accuracy: 0.9937\n",
            "Epoch 9/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0041 - accuracy: 0.9988 - val_loss: 0.0271 - val_accuracy: 0.9932\n",
            "Epoch 10/10\n",
            "1624/1624 [==============================] - 6s 4ms/step - loss: 0.0037 - accuracy: 0.9984 - val_loss: 0.0296 - val_accuracy: 0.9931\n",
            ">#9: 99.435\n",
            "Epoch 1/10\n",
            "1624/1624 [==============================] - 7s 4ms/step - loss: 0.3874 - accuracy: 0.8702 - val_loss: 0.0317 - val_accuracy: 0.9888\n",
            "Epoch 2/10\n",
            "1624/1624 [==============================] - 7s 4ms/step - loss: 0.0217 - accuracy: 0.9931 - val_loss: 0.0216 - val_accuracy: 0.9930\n",
            "Epoch 3/10\n",
            "1624/1624 [==============================] - 6s 4ms/step - loss: 0.0154 - accuracy: 0.9945 - val_loss: 0.0198 - val_accuracy: 0.9935\n",
            "Epoch 4/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0115 - accuracy: 0.9958 - val_loss: 0.0252 - val_accuracy: 0.9926\n",
            "Epoch 5/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0096 - accuracy: 0.9966 - val_loss: 0.0257 - val_accuracy: 0.9929\n",
            "Epoch 6/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0076 - accuracy: 0.9973 - val_loss: 0.0273 - val_accuracy: 0.9928\n",
            "Epoch 7/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0065 - accuracy: 0.9979 - val_loss: 0.0276 - val_accuracy: 0.9927\n",
            "Epoch 8/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0044 - accuracy: 0.9988 - val_loss: 0.0266 - val_accuracy: 0.9936\n",
            "Epoch 9/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0035 - accuracy: 0.9987 - val_loss: 0.0330 - val_accuracy: 0.9919\n",
            "Epoch 10/10\n",
            "1624/1624 [==============================] - 5s 3ms/step - loss: 0.0045 - accuracy: 0.9981 - val_loss: 0.0365 - val_accuracy: 0.9919\n",
            ">#10: 99.384\n",
            "[99.3326485157013, 99.48665499687195, 99.4353175163269, 99.53798651695251, 99.3326485157013, 99.38398599624634, 99.38398599624634, 99.58932399749756, 99.4353175163269, 99.38398599624634]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyqfsdEVauor"
      },
      "source": [
        "Accuracy of CNN :\n",
        "\n",
        "* mean: 99.4353187084198\n",
        "* std: 0.10767958798933588"
      ]
    }
  ]
}